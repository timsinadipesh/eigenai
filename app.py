"""
ONNX Gemma 3n E2B Local Inference Application
Clean implementation that works with locally downloaded ONNX models only
"""

import os
import gc
import json
import logging
import tempfile
import subprocess
import hashlib
from pathlib import Path
from typing import List, Dict, Any, Optional
import numpy as np
import onnxruntime as ort
from fastapi import FastAPI, File, UploadFile, HTTPException, Form
from fastapi.staticfiles import StaticFiles
from fastapi.responses import JSONResponse, FileResponse, StreamingResponse
from transformers import AutoProcessor, AutoConfig
import uvicorn
from PIL import Image
from contextlib import asynccontextmanager

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class PiperTTSEngine:
    """Piper TTS integration for text-to-speech"""
    
    def __init__(self):
        # Find the voices directory relative to this file
        self.voices_dir = Path(__file__).parent / "voices"
        self.cache_dir = Path.home() / ".cache" / "piper_audio"
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # Dynamically find all .onnx voice models in the voices directory
        self.voices = {}
        if self.voices_dir.exists():
            for onnx_file in self.voices_dir.glob("*.onnx"):
                # Example: en_US-lessac-high.onnx ‚Üí "en_US" and "en"
                lang_full = onnx_file.name.split("-")[0]  # "en_US"
                lang_short = lang_full.split("_")[0]      # "en"
                self.voices[lang_full] = onnx_file
                self.voices[lang_short] = onnx_file

        # Check if piper is available
        self.piper_available = self._check_piper_installation()
        
    def _check_piper_installation(self) -> bool:
        """Check if piper is installed and available"""
        try:
            result = subprocess.run(["piper", "--help"], 
                                  capture_output=True, text=True, timeout=5)
            return result.returncode == 0
        except (subprocess.TimeoutExpired, FileNotFoundError):
            logger.warning("Piper TTS not found. TTS functionality disabled.")
            return False
    
    def _detect_language(self, text: str) -> str:
        """Simple language detection based on common words/patterns"""
        text_lower = text.lower()
        
        # French indicators
        french_words = ['le', 'la', 'les', 'de', 'du', 'des', 'un', 'une', 'et', 'est', 'avec', 'pour', 'dans', 'sur']
        if any(word in text_lower.split() for word in french_words):
            if self.voices["fr"].exists():
                return "fr"
        
        # German indicators  
        german_words = ['der', 'die', 'das', 'und', 'ist', 'mit', 'f√ºr', 'in', 'auf', 'von', 'zu', 'den', 'dem']
        if any(word in text_lower.split() for word in german_words):
            if self.voices["de"].exists():
                return "de"
        
        # Default to English
        return "en"
    
    def _get_cache_path(self, text: str, language: str) -> Path:
        """Generate cache file path based on text hash"""
        text_hash = hashlib.md5(text.encode()).hexdigest()
        return self.cache_dir / f"{text_hash}_{language}.wav"
    
    async def synthesize(self, text: str, language: str = None) -> Optional[Path]:
        """Synthesize text to speech using Piper"""
        
        if not self.piper_available:
            return None
        
        # Auto-detect language if not specified
        if language is None:
            language = self._detect_language(text)
        
        # Check if voice model exists
        if language not in self.voices or not self.voices[language].exists():
            logger.warning(f"Voice model for {language} not found, falling back to English")
            language = "en"
            if not self.voices[language].exists():
                logger.error("No voice models available")
                return None
        
        # Check cache first
        cache_path = self._get_cache_path(text, language)
        if cache_path.exists():
            logger.info(f"Using cached audio: {cache_path}")
            return cache_path
        
        try:
            # Clean text for TTS (remove markdown, etc.)
            clean_text = self._clean_text_for_tts(text)
            
            # Run Piper TTS
            cmd = [
                "piper",
                "--model", str(self.voices[language]),
                "--output_file", str(cache_path)
            ]
            
            process = subprocess.Popen(
                cmd,
                stdin=subprocess.PIPE,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            
            stdout, stderr = process.communicate(input=clean_text, timeout=30)
            
            if process.returncode == 0 and cache_path.exists():
                logger.info(f"TTS synthesis complete: {cache_path}")
                return cache_path
            else:
                logger.error(f"Piper TTS failed: {stderr}")
                return None
                
        except subprocess.TimeoutExpired:
            process.kill()
            logger.error("Piper TTS timed out")
            return None
        except Exception as e:
            logger.error(f"TTS synthesis error: {e}")
            return None
    
    def _clean_text_for_tts(self, text: str) -> str:
        """Clean text for better TTS synthesis"""
        import re
        
        # Remove markdown formatting
        text = re.sub(r'\*\*(.*?)\*\*', r'\1', text)  # Bold
        text = re.sub(r'\*(.*?)\*', r'\1', text)      # Italic
        text = re.sub(r'`(.*?)`', r'\1', text)        # Code
        text = re.sub(r'#{1,6}\s', '', text)          # Headers
        
        # Clean up extra whitespace
        text = re.sub(r'\s+', ' ', text).strip()
        
        # Limit length (Piper works better with shorter texts)
        if len(text) > 1000:
            text = text[:1000] + "..."
        
        return text


class LocalONNXGemmaEngine:
    """Local-only ONNX Gemma 3n E2B inference engine"""
    
    def __init__(self, cache_dir: Optional[str] = None):
        # Local cache directory (where download.py puts files)
        if cache_dir:
            self.cache_dir = Path(cache_dir)
        else:
            self.cache_dir = Path.home() / ".cache" / "onnx_models" / "gemma-3n-e2b"
        
        # Model components (will be loaded from local files)
        self.processor = None
        self.config = None
        self.embed_session = None
        self.audio_session = None
        self.vision_session = None
        self.decoder_session = None
        
        # Model state
        self.is_loaded = False
        
        # Configuration values (loaded from local config.json)
        self.num_key_value_heads = None
        self.head_dim = None
        self.num_hidden_layers = None
        self.eos_token_id = 106  # Gemma 3n specific
        self.image_token_id = None
        self.audio_token_id = None
        
        # ONNX Runtime settings optimized for CPU
        self.ort_providers = ['CPUExecutionProvider']
        self.ort_session_options = ort.SessionOptions()
        self.ort_session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
        self.ort_session_options.intra_op_num_threads = 0  # Use all available cores
    
    def _check_local_files(self) -> bool:
        """Check if all required local files exist"""
        
        # Check if cache directory exists
        if not self.cache_dir.exists():
            logger.error(f"Cache directory not found: {self.cache_dir}")
            return False
        
        # Required config files
        config_files = [
            "config.json",
            "tokenizer.json", 
            "tokenizer_config.json",
            "preprocessor_config.json"
        ]
        
        for file in config_files:
            if not (self.cache_dir / file).exists():
                logger.error(f"Missing config file: {file}")
                return False
        
        # Required ONNX model files (specific quantization from download.py)
        onnx_dir = self.cache_dir / "onnx"
        onnx_files = [
            "embed_tokens_quantized.onnx",      # 2.7 GB
            "audio_encoder.onnx",               # 1.4 GB (fp16)
            "vision_encoder_quantized.onnx",    # 0.3 GB  
            "decoder_model_merged_q4.onnx"      # 1.4 GB (q4f16)
        ]
        
        for file in onnx_files:
            file_path = onnx_dir / file
            if file_path.exists():
                size_mb = os.path.getsize(file_path) / (1024 * 1024)
                logger.info(f"‚úì Found {file} ({size_mb:.1f} MB)")
            else:
                logger.error(f"Missing ONNX model: {file}")
                return False
        
        return True
    
    def _load_local_config(self):
        """Load configuration from local files"""
        
        # Load main config
        config_path = self.cache_dir / "config.json"
        with open(config_path, 'r') as f:
            config_dict = json.load(f)
        
        # Extract text config values
        text_config = config_dict.get("text_config", {})
        self.num_key_value_heads = text_config.get("num_key_value_heads")
        self.head_dim = text_config.get("head_dim") 
        self.num_hidden_layers = text_config.get("num_hidden_layers")
        
        # Extract multimodal token IDs
        self.image_token_id = config_dict.get("image_token_id")
        self.audio_token_id = config_dict.get("audio_token_id")
        
        logger.info(f"Config loaded - layers: {self.num_hidden_layers}, kv_heads: {self.num_key_value_heads}")
        logger.info(f"Special tokens - image: {self.image_token_id}, audio: {self.audio_token_id}")
        
        return config_dict
    
    async def load_models(self):
        """Load all model components from local files only"""
        
        if self.is_loaded:
            logger.info("Models already loaded")
            return
        
        logger.info("="*60)
        logger.info("LOADING LOCAL ONNX GEMMA 3N E2B MODELS")
        logger.info("="*60)
        
        # Step 1: Check all files exist
        logger.info("üîç Step 1: Checking local files...")
        if not self._check_local_files():
            raise HTTPException(
                status_code=500, 
                detail="Local model files not found. Run download.py first!"
            )
        logger.info("‚úì All required files found locally")
        
        try:
            # Step 2: Load configuration
            logger.info("üîß Step 2: Loading configuration...")
            config_dict = self._load_local_config()
            
            # Load processor from local files
            self.processor = AutoProcessor.from_pretrained(
                str(self.cache_dir),  # Load from local directory, not model ID
                local_files_only=True  # Force local-only loading
            )
            
            # Load config from local files  
            self.config = AutoConfig.from_pretrained(
                str(self.cache_dir),  # Load from local directory, not model ID
                local_files_only=True  # Force local-only loading
            )
            
            logger.info("‚úì Processor and config loaded from local files")
            
            # Step 3: Load ONNX sessions
            logger.info("ü§ñ Step 3: Loading ONNX models...")
            onnx_dir = self.cache_dir / "onnx"
            
            # Load embed tokens (quantized, 2.7 GB)
            logger.info("   üì• Loading embed_tokens (quantized)...")
            self.embed_session = ort.InferenceSession(
                str(onnx_dir / "embed_tokens_quantized.onnx"),
                sess_options=self.ort_session_options,
                providers=self.ort_providers
            )
            
            # Load audio encoder (fp16, 1.4 GB)  
            logger.info("   üì• Loading audio_encoder (fp16)...")
            self.audio_session = ort.InferenceSession(
                str(onnx_dir / "audio_encoder.onnx"),
                sess_options=self.ort_session_options,
                providers=self.ort_providers
            )
            
            # Load vision encoder (quantized, 0.3 GB)
            logger.info("   üì• Loading vision_encoder (quantized)...")
            self.vision_session = ort.InferenceSession(
                str(onnx_dir / "vision_encoder_quantized.onnx"),
                sess_options=self.ort_session_options,
                providers=self.ort_providers
            )
            
            # Load decoder (q4f16, 1.4 GB)
            logger.info("   üì• Loading decoder (q4f16)...")
            self.decoder_session = ort.InferenceSession(
                str(onnx_dir / "decoder_model_merged_q4.onnx"),
                sess_options=self.ort_session_options,
                providers=self.ort_providers
            )
            
            self.is_loaded = True
            
            logger.info("="*60)
            logger.info("‚úÖ ALL MODELS LOADED SUCCESSFULLY!")
            logger.info("="*60)
            logger.info("üöÄ Ready for inference!")
            
        except Exception as e:
            logger.error(f"Failed to load models: {e}")
            self.cleanup()
            raise HTTPException(status_code=500, detail=f"Model loading failed: {e}")
    
    def cleanup(self):
        """Clean up loaded models"""
        logger.info("üßπ Cleaning up models...")
        
        sessions = [
            ('embed_session', self.embed_session),
            ('audio_session', self.audio_session), 
            ('vision_session', self.vision_session),
            ('decoder_session', self.decoder_session)
        ]
        
        for name, session in sessions:
            if session:
                del session
                setattr(self, name, None)
        
        self.is_loaded = False
        gc.collect()
        logger.info("‚úì Cleanup complete")
    
    async def generate_response(self, messages: List[Dict], max_new_tokens: int = 1000) -> str:
        """Generate response using local ONNX models"""
        
        if not self.is_loaded:
            raise HTTPException(status_code=500, detail="Models not loaded")
        
        # Limit max tokens to prevent excessive generation
        max_new_tokens = min(max_new_tokens, 2048)
        logger.info(f"üéØ Starting generation (max_tokens: {max_new_tokens})")
        
        try:
            # Process input with chat template
            inputs = self.processor.apply_chat_template(
                messages,
                add_generation_prompt=True,
                tokenize=True,
                return_dict=True,
                return_tensors="pt"  # PyTorch tensors required
            )
            
            # Convert to numpy for ONNX
            input_ids = inputs["input_ids"].numpy()
            attention_mask = inputs["attention_mask"].numpy()
            position_ids = np.cumsum(attention_mask, axis=-1) - 1
            
            # Extract multimodal inputs if present
            pixel_values = inputs["pixel_values"].numpy() if "pixel_values" in inputs else None
            input_features = inputs["input_features"].numpy().astype(np.float32) if "input_features" in inputs else None
            input_features_mask = inputs["input_features_mask"].numpy() if "input_features_mask" in inputs else None
            
            logger.info(f"üìä Input shape: {input_ids.shape}")
            if pixel_values is not None:
                logger.info(f"üñºÔ∏è  Image input: {pixel_values.shape}")
            if input_features is not None:
                logger.info(f"üéµ Audio input: {input_features.shape}")
            
            # Initialize generation state
            batch_size = input_ids.shape[0]
            past_key_values = {
                f"past_key_values.{layer}.{kv}": np.zeros(
                    [batch_size, self.num_key_value_heads, 0, self.head_dim], 
                    dtype=np.float32
                )
                for layer in range(self.num_hidden_layers)
                for kv in ("key", "value")
            }
            
            generated_tokens = np.empty((batch_size, 0), dtype=np.int64)
            image_features = None
            audio_features = None
            
            # Generation loop
            for step in range(max_new_tokens):
                # Get embeddings
                inputs_embeds, per_layer_inputs = self.embed_session.run(
                    None, {"input_ids": input_ids}
                )
                
                # Process image features (once)
                if image_features is None and pixel_values is not None:
                    image_features = self.vision_session.run(
                        ["image_features"],  
                        {"pixel_values": pixel_values}
                    )[0]
                    
                    # Replace image tokens with features
                    mask = (input_ids == self.image_token_id)
                    if mask.any():
                        inputs_embeds[mask] = image_features.reshape(-1, image_features.shape[-1])
                
                # Process audio features (once)  
                if audio_features is None and input_features is not None and input_features_mask is not None:
                    audio_features = self.audio_session.run(
                        ["audio_features"],
                        {
                            "input_features": input_features,
                            "input_features_mask": input_features_mask
                        }
                    )[0]
                    
                    # Replace audio tokens with features
                    mask = (input_ids == self.audio_token_id)
                    if mask.any():
                        inputs_embeds[mask] = audio_features.reshape(-1, audio_features.shape[-1])
                
                # Run decoder
                outputs = self.decoder_session.run(None, {
                    "inputs_embeds": inputs_embeds,
                    "per_layer_inputs": per_layer_inputs,
                    "position_ids": position_ids,
                    **past_key_values
                })
                
                logits = outputs[0]
                present_key_values = outputs[1:]
                
                # Sample next token (greedy for now)
                next_token = np.argmax(logits[:, -1], axis=-1, keepdims=True)
                generated_tokens = np.concatenate([generated_tokens, next_token], axis=-1)
                
                # Update for next iteration
                input_ids = next_token
                attention_mask = np.ones_like(input_ids)
                position_ids = position_ids[:, -1:] + 1
                
                # Update key-value cache
                for i, key in enumerate(past_key_values.keys()):
                    past_key_values[key] = present_key_values[i]
                
                # Check for EOS token
                if (next_token == self.eos_token_id).all():
                    logger.info(f"üèÅ EOS reached at step {step}")
                    break
                
                # Progress logging
                if step > 0 and step % 50 == 0:
                    logger.info(f"‚è≥ Generated {step} tokens...")
            
            # Decode the generated tokens
            response = self.processor.batch_decode(
                generated_tokens, 
                skip_special_tokens=True
            )[0]
            
            logger.info(f"‚úÖ Generation complete ({len(generated_tokens[0])} tokens)")
            return response.strip()
            
        except Exception as e:
            logger.error(f"üí• Generation failed: {e}")
            import traceback
            logger.error(f"Traceback: {traceback.format_exc()}")
            raise HTTPException(status_code=500, detail=f"Generation failed: {e}")


# Initialize the inference engine and TTS
inference_engine = LocalONNXGemmaEngine()
tts_engine = PiperTTSEngine()


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan manager"""
    try:
        await inference_engine.load_models()
        logger.info("üéâ Application startup complete!")
        yield
    finally:
        inference_engine.cleanup()
        logger.info("üëã Application shutdown complete")


# FastAPI application
app = FastAPI(
    title="Local ONNX Gemma 3n E2B",
    description="Multimodal AI inference using locally cached ONNX models",
    version="2.0.0",
    lifespan=lifespan
)


app.mount("/static", StaticFiles(directory="static"), name="static")


@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy" if inference_engine.is_loaded else "loading",
        "models_loaded": inference_engine.is_loaded,
        "tts_available": tts_engine.piper_available,
        "cache_location": str(inference_engine.cache_dir),
        "quantization": {
            "embed_tokens": "quantized",
            "audio_encoder": "fp16", 
            "vision_encoder": "quantized",
            "decoder": "q4f16"
        }
    }


@app.get("/chat")
async def chat_interface():
    """Serve the chat interface"""
    return FileResponse("static/chat.html")


@app.post("/api/generate")
async def generate_response(
    text: str = Form(...),
    image: UploadFile = File(None),
    audio: UploadFile = File(None),
    max_tokens: int = Form(1000)
):
    """Generate response from text, image, and/or audio inputs"""
    
    # Limit max tokens
    max_tokens = min(max_tokens, 2048)
    
    content = [{"type": "text", "text": text}]
    temp_files = []
    
    try:
        # Handle image upload
        if image and image.filename:
            if not image.content_type.startswith("image/"):
                raise HTTPException(status_code=400, detail="Invalid image format")
            
            with tempfile.NamedTemporaryFile(delete=False, suffix=".jpg") as tmp_file:
                content_bytes = await image.read()
                tmp_file.write(content_bytes)
                temp_files.append(tmp_file.name)
            
            img = Image.open(temp_files[-1])
            if img.mode != 'RGB':
                img = img.convert('RGB')
            content.append({"type": "image", "image": img})
            logger.info(f"üñºÔ∏è  Image processed: {img.size}")
        
        # Handle audio upload
        if audio and audio.filename:
            if not audio.content_type.startswith("audio/"):
                raise HTTPException(status_code=400, detail="Invalid audio format")
            
            with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp_file:
                content_bytes = await audio.read()
                tmp_file.write(content_bytes)
                temp_files.append(tmp_file.name)
            
            content.append({"type": "audio", "audio": temp_files[-1]})
            logger.info(f"üéµ Audio processed: {temp_files[-1]}")
        
        # Generate response
        messages = [{"role": "user", "content": content}]
        response = await inference_engine.generate_response(messages, max_new_tokens=max_tokens)
        
        return {
            "response": response,
            "input_modalities": {
                "text": True,
                "image": image is not None and image.filename,
                "audio": audio is not None and audio.filename
            },
            "tokens_generated": len(response.split())  # Rough estimate
        }
        
    finally:
        # Clean up temporary files
        for temp_file in temp_files:
            try:
                os.unlink(temp_file)
            except FileNotFoundError:
                pass


@app.post("/api/tts")
async def text_to_speech(
    text: str = Form(...),
    language: str = Form(None)
):
    """Convert text to speech using Piper TTS"""
    
    if not tts_engine.piper_available:
        raise HTTPException(status_code=503, detail="TTS service not available")
    
    try:
        # Synthesize audio
        audio_path = await tts_engine.synthesize(text, language)
        
        if not audio_path or not audio_path.exists():
            raise HTTPException(status_code=500, detail="TTS synthesis failed")
        
        # Stream the audio file
        def audio_stream():
            with open(audio_path, 'rb') as audio_file:
                while chunk := audio_file.read(8192):
                    yield chunk
        
        return StreamingResponse(
            audio_stream(),
            media_type="audio/wav",
            headers={
                "Content-Disposition": "inline; filename=tts_audio.wav",
                "Accept-Ranges": "bytes"
            }
        )
        
    except Exception as e:
        logger.error(f"TTS error: {e}")
        raise HTTPException(status_code=500, detail=f"TTS failed: {e}")


@app.get("/")
async def root():
    """Serve the main index page"""
    return FileResponse("static/index.html")


def main():
    """Run the application"""
    print("="*70)
    print("üöÄ LOCAL ONNX GEMMA 3N E2B MULTIMODAL SERVER")
    print("="*70)
    print("üìÅ Cache directory:", inference_engine.cache_dir)
    print("üîß ONNX Runtime providers:", inference_engine.ort_providers)
    print("üìä Using CPU inference with all available cores")
    print("üéµ TTS Engine:", "Enabled" if tts_engine.piper_available else "Disabled")
    print("="*70)
    print()
    print("Starting server...")
    print("- Health check: http://localhost:8000/health")
    print("- API endpoint: http://localhost:8000/api/generate")
    print("- TTS endpoint: http://localhost:8000/api/tts")
    print("- Interactive docs: http://localhost:8000/docs")
    print()
    
    # Run the FastAPI server
    uvicorn.run(
        app,
        host="127.0.0.1",
        port=8000,
        log_level="info",
        access_log=True,
    )


if __name__ == "__main__":
    main()
